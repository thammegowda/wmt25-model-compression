{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5766cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# tarball from email, extracted here\n",
    "root = Path(\"../tmp/eval-backups-02-v2\")\n",
    "assert root.exists()\n",
    "\n",
    "submissions = {k.name.replace(\"eval02-\", \"\"):k for k in root.glob(\"eval02-*\") if k.is_dir()}\n",
    "submissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import lru_cache\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_ID = \"CohereLabs/aya-expanse-8b\"\n",
    "\n",
    "EXCLUSIONS = {\n",
    "    \"aya-expanse-8b-bnb-4bit\", # dupe: same as aya-expanse-8b-bnb-4bit-fp4\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=8)\n",
    "def get_tokenizer(model_name=MODEL_ID):\n",
    "    return AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_output_token_count(stat_file):\n",
    "    stats = json.loads(stat_file.read_text())\n",
    "\n",
    "    if \"output_token_count\" in stats:\n",
    "        return stats[\"output_token_count\"]\n",
    "    out_file = stat_file.parent / (stat_file.name.replace(\".stats.json\", \"\"))\n",
    "    assert out_file.exists(), f\"no out file {out_file}\"\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    tok_count = 0\n",
    "    with out_file.open() as lines:\n",
    "        lines = [line for line in lines if line.strip()]\n",
    "        for line in tqdm(lines, desc=f\"tokenizing {out_file.stem}\", unit=\" lines\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokenized = tokenizer(line, return_tensors=\"pt\")\n",
    "            tok_count += tokenized[\"input_ids\"].size(1)\n",
    "\n",
    "    stats[\"output_token_count\"] = tok_count\n",
    "    stat_file.write_text(json.dumps(stats, indent=2))\n",
    "\n",
    "    return stats[\"output_token_count\"]\n",
    "\n",
    "all_stats = []\n",
    "for sub_name, sub_dir in submissions.items():\n",
    "    stat_files = list(sub_dir.glob(\"*/*.stats.json\"))\n",
    "    assert stat_files, \"no stat files\"\n",
    "    for stat_file in stat_files:\n",
    "        name = stat_file.stem\n",
    "        # wmt25.ces-deu.deu.aya-expanse-8b-bnb-4bit-fp4.out.batch256.run1.stats\n",
    "        # (testname).(src)-(tgt).(tgt).(model).out.batch(batch).run(run).stats\n",
    "        # use regex to parse this\n",
    "        m = re.match(r\"(?P<testname>[^.]+)\\.(?P<src>[^-]+)-(?P<tgt>[^.]+)\\.(?P=tgt)\\.(?P<model>.+)\\.out\\.batch(?P<batch>\\d+)\\.run(?P<run>\\d+)\\.stats\", name)\n",
    "        assert m, f\"cannot parse {name}\"\n",
    "        d = m.groupdict()\n",
    "        if d['model'] in EXCLUSIONS:\n",
    "            print(f\"Skipping excluded model {d['model']}\")\n",
    "            continue\n",
    "        if d['testname'] != \"wmt25\":\n",
    "            print(f\"Skipping testname {d['testname']}\")\n",
    "            continue\n",
    "\n",
    "        d['batch'] = int(d['batch'])\n",
    "        d['run'] = int(d['run'])\n",
    "        with stat_file.open() as f:\n",
    "            try:\n",
    "                stats = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {stat_file}: {e}\\nSkipping this file.\")\n",
    "                continue\n",
    "        res = d\n",
    "        #print(\"id\", d, \"stats:\\n\", json.dumps(stats, indent=2))\n",
    "        d[\"time\"] = stats[\"wall_time_sec\"]\n",
    "        d[\"participant\"] = sub_name\n",
    "        d[\"output_token_count\"] = get_output_token_count(stat_file)\n",
    "        # warmup time from stats of testnamed warmup\n",
    "        # warmup tests was run for ces-deu only and batch1 only\n",
    "        warmup_glob = f\"ces-deu/warmup.ces-deu.deu.{d['model']}.out.batch1.run*.stats.json\"\n",
    "        warmup_stats = list(sub_dir.glob(warmup_glob))\n",
    "        if warmup_stats:\n",
    "            # average time\n",
    "            warmups = []\n",
    "            for wf in warmup_stats:\n",
    "                wstats = json.loads(wf.read_text())\n",
    "                warmups.append(wstats[\"wall_time_sec\"])\n",
    "            if len(warmups) < 3:\n",
    "                print(f\"Warning: only {len(warmups)} warmup stats found {warmup_stats}\")\n",
    "            d[\"warmup_time\"] = sum(warmups) / len(warmups)\n",
    "        else:\n",
    "            #print(f\"No warmup stats found {sub_dir}/{warmup_glob}; setting warmup_time to None\")\n",
    "            #d[\"warmup_time\"] = None\n",
    "            raise Exception(f\"no warmup stats found {sub_dir}/{warmup_glob}\")\n",
    "\n",
    "        t2 = stats['end_timestamp'] - stats['start_timestamp']\n",
    "        if abs(t2 - d[\"time\"]) > 1e-3:\n",
    "            raise Exception(f\"time mismatch {d['time']} vs {t2}\")\n",
    "\n",
    "        d['output_rate_all'] = d['output_token_count'] / d['time']\n",
    "        d['output_rate_excl_warmup'] = d['output_token_count'] / (d['time'] - d['warmup_time'])\n",
    "\n",
    "        #print(json.dumps(d))\n",
    "        all_stats.append(d)\n",
    "\n",
    "df = pd.DataFrame(all_stats)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a49b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ca661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Filter\n",
    "df_filtered = df[(df.src == \"ces\") & (df.tgt == \"deu\")]\n",
    "\n",
    "def plot(df_mean, name=\"Output Rate\", figsize=(9,5)):\n",
    "    for testname, df_t in df_mean.groupby(\"testname\"):\n",
    "        plt.figure(figsize=figsize)\n",
    "        for (participant, model), df_m in df_t.groupby([\"participant\", \"model\"]):\n",
    "            df_m_sorted = df_m.sort_values(\"batch\")\n",
    "            label = f\"{participant}:{model}\"\n",
    "            plt.errorbar(\n",
    "                df_m_sorted[\"batch\"],\n",
    "                df_m_sorted[f\"output_rate_mean\"],\n",
    "                yerr=df_m_sorted[f\"output_rate_std\"],\n",
    "                marker='o',\n",
    "                capsize=3,\n",
    "                linestyle='-',\n",
    "                label=label\n",
    "            )\n",
    "        plt.title(f\"{testname} - Output rate vs batch size (mean Â± std)\")\n",
    "        plt.xlabel(\"Batch size\")\n",
    "        plt.ylabel(f\"{name} (tokens/s)\")\n",
    "        plt.xscale(\"log\")\n",
    "        plt.legend(fontsize=\"small\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        # xaxis ticks as 2^n\n",
    "        ax = plt.gca()\n",
    "        ticks = [2**i for i in range(0, 11)]\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticklabels([str(t) for t in ticks])\n",
    "        ax.set_xlim(min(ticks), max(ticks))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # save as pdf\n",
    "\n",
    "        name_simpl = name.lower().strip()\n",
    "        for char in \" _()\":\n",
    "            name_simpl = name_simpl.replace(char, \"-\")\n",
    "        name_simpl = name_simpl.replace(\"--\", \"-\")\n",
    "        plt.savefig(f\"{name_simpl}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# Group: mean + std + count\n",
    "group_cols = ['testname', 'src', 'tgt', 'model', 'batch', 'participant']\n",
    "for field, name in {\n",
    "    \"output_rate_all\": \"Output Rate\",\n",
    "    \"output_rate_excl_warmup\": \"Output Rate Excluding Warmup\"\n",
    "    }.items():\n",
    "    df_mean = df_filtered.groupby(group_cols, as_index=False)\\\n",
    "            .agg(\n",
    "                output_rate_mean=(field, 'mean'),\n",
    "                output_rate_std=(field, 'std'),\n",
    "                output_rate_n_runs=(field, 'size')\n",
    "            )\n",
    "\n",
    "    df_mean = df_mean.sort_values(\n",
    "        [\"testname\", \"src\", \"tgt\", \"batch\", \"participant\", \"model\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    plot(df_mean, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a53a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelzip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
